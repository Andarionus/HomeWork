# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР
Отчет по лабораторной работе #5 выполнил(а):
- Галимуллин Андрей Александрович
- РИ-230932
- Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.


## Задание 1
###  Поиск агентом объекта на сцене.

Ход работы:
- Был создан 3D проект на юнити, к нему при помощи anaconda prompt добавлен ML агент. На данной сцене проведены исследования обучения модели по поиску объекта.
![изображение](https://github.com/user-attachments/assets/2d7f0224-e6fd-43f3-8848-ed4a6f29138b)
1) При запуске сцены было проведено тестирование обучения модели. Анаконда промт сохранил результаты. Они были использованы для работы агента без запуска программы в промте.
![изображение](https://github.com/user-attachments/assets/25ab3d65-4169-4c50-916d-30479d11f074)
![изображение](https://github.com/user-attachments/assets/7d40a092-f20d-444c-aea8-45cf08b3c4d2)
![изображение](https://github.com/user-attachments/assets/2db5f5cc-8161-44e9-8623-cd066cc4ebd7)
![изображение](https://github.com/user-attachments/assets/9a2cc70d-1988-492c-be7c-6ae52f2a451c)

Обучение заняло приличное количество времени, однако модель с присоединённым файлом результатов теперь передвигается самостоательно, хоть и с небольшими ошибками (иногда объект всё же не вписывается траекторией своего движения и падает за пределы опытной зоны)

3) Были сделаны 3, 9 и 27 копий объекта для дальнейших исследований. 
![изображение](https://github.com/user-attachments/assets/8df8b248-c212-4053-95e5-92a7c3054030)
![изображение](https://github.com/user-attachments/assets/4583d7a0-7469-4d04-a2d0-ecef4f5bc6fd)
![изображение](https://github.com/user-attachments/assets/f9618fe6-c4c4-4fde-a66d-39d363eb1458)
![изображение](https://github.com/user-attachments/assets/7c321080-9a1c-4000-8c37-30481a4e928c)

Однако несмотря на большее количество обучаемых моделей, при большем количество записанных успешнных данных, модель всё равно продолжала периодически сваливаться с опытной зоны. Возможно с ещё большей частотой, чем при единичном обучении.

![изображение](https://github.com/user-attachments/assets/6ed0b80a-53c9-4cc1-892e-bdbdebfbe340)
![изображение](https://github.com/user-attachments/assets/1852f4be-c8bc-494a-8b7e-c12bd3f8ee2e)
4) По итогу было выяснено, что большее количество обучающихся моделей лишь ускоряют процесс обучения (по сравнению с 1 моделью, 27 выдадут за считанные секунды первый результат). В то же время это не влиет на качество самого обучения, что нужно конпенсировать более чёткими критериями обучения в скрипте (на мой взгляд, так можно было бы решить проблемы с физикой движения)


## Задание 2
###  Симулятор добычи ресурсов
Ход работы:
- В первую очередь был добавлен проект Unity ML-Agent_EconomicModel и привязано к нему пространство в промте. Ввиду низкой производительности (остуствие видеокарты от nvidia), количество обучающихся моделей было уменьшено (в данном случае обучались 4 и 8 моделей). Далее было произведено наблюдение за обучением модели и выгрузка графиков.
![изображение](https://github.com/user-attachments/assets/4a555504-2981-426b-b91d-edce8f38f825)
![изображение](https://github.com/user-attachments/assets/045e3153-df7c-4015-866e-46fd2a9f3807)
![изображение](https://github.com/user-attachments/assets/00f0867e-ddb9-4619-8a0a-b19b98aaf4b2)
![изображение](https://github.com/user-attachments/assets/8ac0d358-ba54-4e9b-a1e8-e5fdc04e67ac)
![изображение](https://github.com/user-attachments/assets/7953a284-d924-45ab-94f0-80c2de6ee1e8)
![изображение](https://github.com/user-attachments/assets/32c876c1-4874-408f-b485-b5dc8f835d4a)

Всё так же 8 моделей обучаются быстрее 4, результаты в графиках так же отличаются. Кроме того, был исследован скрипт файл передвижения и yaml файл для дальнейших заключений о работе модели.

#### 1. Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.
- Ближайшим аналогом “коэффициента корреляции” является переменная tempInf, в которой сохранется процентное изменение цены на золото между первым и вторым месяцем (tempInf = ((pricesMonth[1] - pricesMonth[0]) / pricesMonth[0]) * 100;). В моём понимании - с каждой разом модель должна приходить с золотом ниже некоторой границы, которая высчитывается как увеличенное количество золота за прошлый месяц на 6%. если условие удовлетворено, то агент получает положительное вознаграждение и сохраняет данную стратегию действий. В обратном же случае он получает отрицательное вознаграждение. Таким образом, переменная является критерием успешности выполнения задачи и помогает обучать модель оптимизировать свои действия для получения положительных вознаграждений и минимизации отрицательных. Переменная tempInf в данном контексте измеряет процентное изменение между ценами на золото в два разных месяца и может рассматриваться как нечто подобное коэффициенту корреляции, если рассматривать изменение как зависимость между двумя переменными (через tempInf считается, конечно, не статическая переменная, поэтому он ближе к темпу роста, чем к критерию Пирсона)
![изображение](https://github.com/user-attachments/assets/651e87d2-1a1b-483a-9036-49b51b1ab073)


#### 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
- Параметры из YAML-файла описывают настройки для обучения модели с использованием метода PPO (Proximal Policy Optimization). В частности изменяя данные в графе hyperparameters (отвечает за отпимизацию обучения модели) можно добиться другого процесса обучения модели. Суть этих параметров можно понять и из их английского названия, однако с некоторыми пришлось залезть в сеть. Далее представлено описание переменных из графы hyperparameters:
1) batch_size: 1024 — размер батча, который используется для сбора данных во время каждой итерации обучения. Чем больше размер батча, тем больше данных будет обработано за один шаг обучения.
2) buffer_size: 10240 — размер буфера для хранения собранных данных (действий агента, я полагаю). Больший буфер позволяет агенту использовать больше данных для обучения (накапливает опыт).
3) learning_rate: 3.0e-4 — скорость обучения (насколько сильно изменяются параметры за итерацию).
4) learning_rate_schedule: linear — стратегия изменения скорости обучения. В данном случае используется линейное снижение скорости обучения по мере тренировки (поможет модели тренироваться в дальнейшем)
5) beta: 1.0e-2 — коэффициент для регуляризации, который используется для контроля величины изменений в политике агента. Это помогает избежать чрезмерных изменений в политике, что может привести к нестабильности.
6) epsilon: 0.2 — параметр, который контролирует величину допустимого изменения политики на каждом шаге обучения. Этот параметр важен для сохранения стабильности в PPO: он ограничивает, насколько сильно может измениться политика в одном шаге.
7) lambd: 0.95 — коэффициент, который используется в Generalized Advantage Estimation (GAE) для вычисления преимущества (advantage) агента. Это значение помогает вычислить более точную оценку вознаграждения от действий агента.
8) num_epoch: 3 — количество эпох, которые агент проходит для каждой итерации обучения (используется для обновления параметров через итерации).


#### 3. Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 
- Хоть оба варианта мл-агентов представленных в работе довольно тривиальны в своих механиках, я могу выделить один из вариантов игровых ситуаций, где данные агенты могли бы быть использованы. Для симуляторов, в частности городостроительных. В них игрок не имеет прямого контроля над жителями города, как в других стратегиях, поэтому агент обученный на взаимодействие с экономикой мог случайным образом разбавлять её состояние для игрока. Ибо, хоть пример и предствален с золотой шахтой, на самом деле такой агент может собирать свою награду как в офисе в виде зарплаты, так и любое другое сырьё. Кроме того, заранее олученные модели агента могли быть отвенственны за разные стадии развития города (например, мы обучили по-разному несколько моделей, которые в дальнейшем имели бы разный уровень максимального дохода). Первый же пример мог бы использоваться для сопроводительных квестов в РПГ-подобных задачах. Как показано в лекции, агент может перемещаться между заранее заданными точками, симулируя таким образом дейтельность NPC в определённом пространстве.
- Однако стоит выделить, что всё же для последнего примера можно написать и программную реализацию не менее просто, чем обучить агента. Поэтому мл-агент проще проявляет себя в ситуациях, где обрабатывается множество различных переменных (в примере с экономикой шахты агент взаимодействует с той же стоимостью своей кирки, например) или где стоит нетривиальная задача. Например, на хабре выложено обучение команды футболистов (игра сама с собой). Благодаря такому примеру можно просто автоматизировать процесс разработки каких-либо симуляторов (от спортивных, до тех же городских).
#### 4. Геймдизайн игры Mario (найдено в лекции).
- Я полагаю, что в данном случае идёт речь об оригинальной игре, а не о современных переосмыслениях. В данном случае можно было бы обучить сущности врагов при помощи мл-агента, чтобы усложнить игровой процесс. В базовой игре противники перемещаются в основном в одной плоскости (или вовсе только атакуют как Боузер) между определёнными двумя точками. Мы могли бы переделать перемещение этих сущностей так, чтобы они неординарно взаимодействовали с игроком. Одни могли бы прокладывать путь и следовать за игровым персонажем, в то время как другие могли бы специально избегать прыжков от Марио (чего-то подобного не хватает и в поведении сущности противников из СПАСТИ-РТФ).



## Задание 3
### В задании явно не выделено 3е задание, поэтому дальше пойдут мои выводы.
#### Выводы
- Было создано собственно виртуально пространство по работе с мл-агентом, стали понятны приципы работы anaconda promt. Научен подключать параметры агента к unity и проведено обучение двух примеров из задания. В конечном итоге видны большие возможности для пременения, по сравнению с перцептроном из прошлого задания (для нетривиальных примеров). Далее будет продуман собственны проект по использованию агента в связке с другими лабораторными работами на данном курсе. 

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
